# RPVM 方法实验需求文档

## 文档概述
本需求文档基于提供的`PVM.md`方法（Reflective Plan-Verify Memory, 简称RPVM）设计一个完整的RAG（Retrieval-Augmented Generation）实验实现。RPVM的核心是“反思-行动-记忆”（Reflect–Act–Remember）的迭代推理框架，用于解决多跳问题（如多跳QA）。该方法通过先规划推理链（Plan）、再检索验证（Verify）、并更新记忆（Memory）的方式，避免重复检索和上下文冗余。

实验将在FlashRAG框架中实现，利用其组件化设计。关键约束：
- **Retriever**：使用E5模型（稠密检索器）。
- **所有LLM（包括Planner、Verifier和最终生成器）**：统一使用OpenAI API，模型为ChatGPT-3.5（gpt-3.5-turbo）。
- **数据集**：HotpotQA 和 2WikiMultihopQA（均为多跳QA数据集）。
- **知识源**：使用维基百科作为文档库（corpus），基于FlashRAG提供的预处理脚本构建索引。
- **目标**：实现RPVM的完整Pipeline，并在指定数据集上运行实验，评估性能。

Code Agent需基于FlashRAG的API（如Config、Retriever、Generator、Pipeline等）实现。实验输出包括：中间结果（计划链、验证结果、记忆更新）和最终答案，以及评估指标。

---

## 总体思路（核心理念）
**目标**：构建一个迭代推理框架，用于多跳问题求解。通过先规划整体推理链，再逐一验证并更新记忆，避免直接生成-修正的低效循环。

**核心创新点**（基于PVM.md）：
1. 先推理出整体计划链（plan），再逐步验证。
2. 通过验证过的plan文本拼接形成短期记忆，支持下一轮规划。
3. 当检索不到证据时，通过rewrite + 扩大检索强化召回。
4. 无需复杂权重或多级记忆，保持简单可复现。

**适用场景**：多跳QA，如HotpotQA（桥接式多跳）和2WikiMultihopQA（复杂推理链）。

**预期效果**：在多跳问题上，提高准确率，减少检索次数和上下文长度。

---

## 整体流程框架
RPVM框架包括三个核心模块：Reflective Planner、Plan Verifier 和 Memory Updater。流程是一个循环（max_iter=5），直到记忆足够或达到上限。

### 初始化
- **输入**：原始问题 Q。
- **内存 M**：初始为空字符串（文本形式）。
- **参数**：
  - max_iter = 5（最大循环次数）。
  - max_retrieval_attempts = 2（每个plan的最大检索尝试次数）。
  - retrieval_topk = 5（每次检索返回的文档数）。

### 核心模块
1. **Reflective Planner（反思规划器）**：
   - **输入**：Q, M。
   - **输出**：计划链 plans = [plan₁, plan₂, ...]（列表，每项为自然语言断言）。
   - **实现**：基于LLM（OpenAI GPT-3.5）生成。Prompt逻辑：结合M分析Q，若M足够则输出"ANSWER_READY"；否则生成逻辑推理链。
   - **FlashRAG整合**：使用Generator的generate方法，输入prompt模板（需自定义）。

2. **Plan Verifier（验证模块）**：
   - **输入**：当前planₖ, Q, M。
   - **流程**：
     - 使用Retriever（E5）检索相关文档docs（query=planₖ）。
     - 若docs为空：rewrite query（用LLM改写plan为更精确检索词），重试（最多max_retrieval_attempts次）。
     - 基于docs，让LLM判断verdict：supported / contradicted / insufficient。
     - 输出：verdict, corrected_plan（若contradicted则修正）, evidence（精炼证据）。
   - **FlashRAG整合**：Retriever.batch_search方法检索；Generator生成verdict和修正。

3. **Memory Updater（记忆更新）**：
   - **输入**：verdict, corrected_plan。
   - **流程**：
     - supported：M += "\n{corrected_plan} (verified)"
     - contradicted：M += "\n{corrected_plan} (corrected)" 并短路当前轮（break循环）。
     - insufficient：不更新，继续下一个plan。
   - **实现**：简单字符串拼接，无需结构化。

### 完整伪代码（Python风格）
```python
from flashrag import Config, get_retriever, get_generator  # 导入FlashRAG组件

# 初始化Config、Retriever (E5)、Generator (OpenAI GPT-3.5)
config = Config(...)  # 详见配置部分
retriever = get_retriever(config)
generator = get_generator(config)

def rpvm_pipeline(Q):
    M = ""
    for i in range(config.max_iter):
        plans = planner(generator, Q, M)  # 使用Generator生成plans

        if plans == "ANSWER_READY":
            return generate_final_answer(generator, Q, M)

        for plan in plans:
            for attempt in range(config.max_retrieval_attempts):
                docs = retriever.batch_search([plan], top_k=config.retrieval_topk)  # 检索
                if not docs:
                    plan = rewrite_retrieval(generator, plan)  # 用Generator改写
                    continue

                verdict, corrected_plan, evidence = verify(generator, plan, docs)  # 用Generator验证

                if verdict == "supported":
                    M += f"\n{corrected_plan} (verified)"
                    break
                elif verdict == "contradicted":
                    M += f"\n{corrected_plan} (corrected)"
                    break  # 短路当前轮
                else:  # insufficient
                    if attempt == config.max_retrieval_attempts - 1:
                        pass  # 放弃
    return generate_best_effort_answer(generator, Q, M)  # 尽力回答
```

### 终止条件
- Planner输出"ANSWER_READY"：生成最终答案。
- 达到max_iter：输出best-effort answer，并标记不确定性。

---

## 配置设置（基于FlashRAG Config）
使用FlashRAG的Config类管理参数。提供YAML模板和字典形式。优先使用字典覆盖YAML。

### YAML配置文件模板（my_rpvm_config.yaml）
```yaml
# 基本设置 (参考basic-settings.md)
data_dir: "path/to/FlashRAG_Data/"  # 数据集路径
save_dir: "output/rpvm_experiments/"
seed: 2024
save_intermediate_data: True
save_note: "rpvm_hotpotqa_2wikimhqa"

# 数据集 (参考evaluation-datasets.md)
dataset_name: ["hotpotqa", "2wikimultihopqa"]
split: ["test"]

# 检索器设置 (参考retriever-settings.md)
retrieval_method: "e5"  # 使用E5
retrieval_model_path: "path/to/e5-model"  # E5模型路径 (e.g., intfloat/e5-base-v2)
index_path: "path/to/e5_index"  # 预构建E5索引 (基于wiki corpus, 参考build-index.md)
corpus_path: "path/to/wiki_corpus.jsonl"  # 维基百科文档库 (参考build-corpus.md)
retrieval_topk: 5
retrieval_batch_size: 32
use_fp16: True

# 生成器设置 (参考generator-setting.md)
framework: "openai"
generator_model: "gpt-3.5-turbo"
openai_setting:
  api_key: "your_openai_api_key"
  base_url: "https://api.openai.com/v1"
generator_max_input_len: 4096
generator_batch_size: 1
generation_params:
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9

# 自定义RPVM参数
max_iter: 5
max_retrieval_attempts: 2

# 评测设置 (参考basic-settings.md)
metrics: ["em", "f1", "acc", "retrieval_recall"]
metric_setting:
  retrieval_recall_topk: 5
  tokenizer_name: "gpt-3.5-turbo"
save_metric_score: True
```

### 在代码中加载Config
```python
from flashrag.config import Config
config_dict = {
    'max_iter': 5,  # 覆盖YAML
    # 其他覆盖参数
}
config = Config(config_file_path='my_rpvm_config.yaml', config_dict=config_dict)
```

### 提示模板（Prompt Templates）
自定义prompt模板（参考generator.md的prompt构建章节）。在代码中定义为字典或字符串：
- **Planner Prompt**： "结合记忆{M}，分析问题{Q}。若足够回答，直接输出'ANSWER_READY'；否则生成计划链：[plan1, plan2, ...]"
- **Verifier Prompt**： "基于文档{docs}，判断plan{plan}：supported/contradicted/insufficient。若contradicted，修正为正确版本。"
- **Rewrite Prompt**： "改写{plan}为更精确的检索查询。"
- **Final Answer Prompt**： "基于记忆{M}，回答{Q}。"

---

## 数据集和评估
- **数据集**：
  - HotpotQA：多跳QA，桥接式问题。加载test split (# Test: ~7,405)。
  - 2WikiMultihopQA：复杂多跳推理。加载test split (# Test: ~12,576)。
  - 使用FlashRAG的get_dataset(config)加载（参考evaluation-datasets.md）。

- **文档库和索引**：
  - Corpus：使用维基百科2018版本（参考build-corpus.md），处理为jsonl格式。
  - Index：使用E5构建Flat索引（参考build-index.md的稠密检索部分）。

- **评估**：
  - 指标：EM (exact match), F1, ACC (accuracy), Retrieval Recall。
  - 运行：在Pipeline中集成评估，保存到save_dir下的metric_score.txt。
  - 基准：比较基线RAG（无RPVM）和RPVM的性能（准确率、检索次数、token消耗）。

---

## 实现要求
- **代码结构**：
  - 主文件：rpvm_pipeline.py，包含rpvm_pipeline函数。
  - 模块函数：planner(generator, Q, M), verify(generator, plan, docs), rewrite_retrieval(generator, plan), generate_final_answer(generator, Q, M)等。
  - 集成FlashRAG：使用get_retriever, get_generator；避免自定义组件，除非必要（可继承Pipeline类）。
  - 处理批量：支持batch处理多个Q（使用Generator.batch_generate）。

- **错误处理**：
  - 检索失败：重试后标记insufficient。
  - API限速：添加retry机制（OpenAI SDK支持）。
  - 内存溢出：限制M长度（若>3000 tokens，摘要M）。

- **测试与输出**：
  - 示例运行：对HotpotQA的5个样本运行，打印中间plans、verdict、M。
  - 保存：intermediate_data.jsonl（每个Q的plans、M、final_answer）。
  - 完整实验：对test split运行，输出平均指标。

- **依赖**：
  - FlashRAG安装（参考installation.md）。
  - OpenAI API密钥有效。
  - E5模型和索引预构建。

- **时间/资源估计**：
  - 单样本：~10-20s（取决于iter和API延迟）。
  - 全数据集：使用batch_size优化，预计数小时（多GPU可选）。

Code Agent需严格遵循此文档实现。若有疑问，参考FlashRAG docs（如retriever.md, generator.md）。完成后，提供完整代码和运行示例。