# RPVMæ–¹æ³•ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
3. [è¿è¡Œå®éªŒ](#è¿è¡Œå®éªŒ)
4. [ç»“æœåˆ†æ](#ç»“æœåˆ†æ)
5. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
6. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: ç¯å¢ƒå‡†å¤‡

```bash
# ç¡®ä¿åœ¨flashRAGç¯å¢ƒä¸­
conda activate flashRAG  # æˆ–ä½ çš„ç¯å¢ƒå

# å®‰è£…å¿…è¦çš„ä¾èµ–
pip install openai tiktoken
```

### æ­¥éª¤2: é…ç½®OpenAI API

```bash
# Linux/Mac
export OPENAI_API_KEY='your-api-key-here'

# Windows PowerShell
$env:OPENAI_API_KEY='your-api-key-here'

# Windows CMD
set OPENAI_API_KEY=your-api-key-here
```

### æ­¥éª¤3: é…ç½®æ–‡ä»¶è·¯å¾„

ç¼–è¾‘ `rpvm_config.yaml`ï¼š

```yaml
# ä¿®æ”¹ä»¥ä¸‹è·¯å¾„ä¸ºä½ çš„å®é™…è·¯å¾„
data_dir: "D:/path/to/your/dataset"  # FlashRAGæ•°æ®ç›®å½•
retrieval_model_path: "D:/path/to/e5-base-v2"  # E5æ¨¡å‹è·¯å¾„
index_path: "D:/path/to/e5_Flat.index"  # E5ç´¢å¼•è·¯å¾„
corpus_path: "D:/path/to/general_knowledge.jsonl"  # è¯­æ–™åº“è·¯å¾„
```

### æ­¥éª¤4: æµ‹è¯•è¿è¡Œ

```bash
cd RPVM

# è¿è¡ŒåŸºæœ¬æµ‹è¯•
python test_rpvm.py

# è¿è¡Œç®€å•ç¤ºä¾‹
python simple_example.py --mode simple
```

---

## âš™ï¸ è¯¦ç»†é…ç½®

### é…ç½®æ–‡ä»¶ç»“æ„

`rpvm_config.yaml` åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

#### 1. åŸºæœ¬è®¾ç½®

```yaml
data_dir: "dataset"           # æ•°æ®é›†æ ¹ç›®å½•
save_dir: "RPVM/output"       # è¾“å‡ºç›®å½•
seed: 2024                    # éšæœºç§å­
save_intermediate_data: True  # æ˜¯å¦ä¿å­˜ä¸­é—´æ•°æ®
gpu_id: "0"                   # GPUè®¾å¤‡IDï¼Œnullè¡¨ç¤ºä½¿ç”¨CPU
```

#### 2. æ•°æ®é›†è®¾ç½®

```yaml
dataset_name: "hotpotqa"  # æ”¯æŒ: hotpotqa, 2wikimultihopqa
split: ["test"]           # æ•°æ®é›†åˆ†å‰²: train, dev, test
```

#### 3. æ£€ç´¢å™¨è®¾ç½® (E5)

```yaml
retrieval_method: "e5"
retrieval_model_path: "intfloat/e5-base-v2"  # æœ¬åœ°è·¯å¾„æˆ–HuggingFace ID
index_path: "indexes/e5_Flat.index"          # FAISSç´¢å¼•æ–‡ä»¶
corpus_path: "indexes/general_knowledge.jsonl"  # æ–‡æ¡£è¯­æ–™åº“
retrieval_topk: 5              # æ¯æ¬¡æ£€ç´¢è¿”å›æ–‡æ¡£æ•°
retrieval_batch_size: 32       # æ£€ç´¢æ‰¹æ¬¡å¤§å°
retrieval_use_fp16: True       # ä½¿ç”¨FP16åŠ é€Ÿ
```

#### 4. ç”Ÿæˆå™¨è®¾ç½® (OpenAI)

```yaml
framework: "openai"
generator_model: "gpt-3.5-turbo"  # æ”¯æŒ: gpt-3.5-turbo, gpt-4, etc.
openai_setting:
  api_key: null                   # APIå¯†é’¥(nullè¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–)
  base_url: "https://api.openai.com/v1"  # APIåŸºç¡€URL
generator_batch_size: 1           # ç”Ÿæˆæ‰¹æ¬¡å¤§å°
generation_params:
  max_tokens: 512                 # æœ€å¤§ç”Ÿæˆtokenæ•°
  temperature: 0.7                # ç”Ÿæˆæ¸©åº¦
  top_p: 0.9                      # nucleus samplingå‚æ•°
```

#### 5. RPVMç‰¹å®šå‚æ•°

```yaml
rpvm_config:
  max_iter: 5                      # æœ€å¤§è¿­ä»£æ¬¡æ•°
  max_retrieval_attempts: 2        # æ¯ä¸ªplanæœ€å¤§æ£€ç´¢é‡è¯•æ¬¡æ•°
  retrieval_topk: 5                # æ¯æ¬¡æ£€ç´¢è¿”å›æ–‡æ¡£æ•°
  memory_max_tokens: 3000          # è®°å¿†æœ€å¤§tokené™åˆ¶
  enable_memory_summary: True      # å¯ç”¨è®°å¿†è‡ªåŠ¨æ‘˜è¦
  planner_temperature: 0.7         # è§„åˆ’å™¨æ¸©åº¦(æ§åˆ¶è®¡åˆ’ç”Ÿæˆçš„éšæœºæ€§)
  verifier_temperature: 0.3        # éªŒè¯å™¨æ¸©åº¦(è¾ƒä½=æ›´ä¿å®ˆçš„åˆ¤æ–­)
  final_answer_temperature: 0.5    # æœ€ç»ˆç­”æ¡ˆæ¸©åº¦
```

### å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `max_iter` | 3-7 | ç®€å•é—®é¢˜ç”¨3-5ï¼Œå¤æ‚é—®é¢˜ç”¨5-7 |
| `max_retrieval_attempts` | 2-3 | å¢åŠ å¯æé«˜å¬å›ï¼Œä½†ä¼šå¢åŠ APIè°ƒç”¨ |
| `retrieval_topk` | 5-10 | å¢åŠ å¯æä¾›æ›´å¤šè¯æ®ï¼Œä½†ä¼šå¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ |
| `planner_temperature` | 0.5-0.9 | è¾ƒé«˜å€¼äº§ç”Ÿæ›´å¤šæ ·çš„è®¡åˆ’ |
| `verifier_temperature` | 0.1-0.5 | è¾ƒä½å€¼ä½¿éªŒè¯æ›´ä¸¥æ ¼ |
| `memory_max_tokens` | 2000-4000 | æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£è°ƒæ•´ |

---

## ğŸ¯ è¿è¡Œå®éªŒ

### 1. å°è§„æ¨¡æµ‹è¯•

å…ˆåœ¨å°‘é‡æ ·æœ¬ä¸Šæµ‹è¯•ï¼Œç¡®ä¿é…ç½®æ­£ç¡®ï¼š

```bash
# æµ‹è¯•5ä¸ªæ ·æœ¬
python run_rpvm_exp.py \
    --dataset_name hotpotqa \
    --split test \
    --num_samples 5 \
    --gpu_id 0
```

### 2. å®Œæ•´å®éªŒ

#### HotpotQAæ•°æ®é›†

```bash
python run_rpvm_exp.py \
    --dataset_name hotpotqa \
    --split test \
    --gpu_id 0
```

#### 2WikiMultihopQAæ•°æ®é›†

```bash
python run_rpvm_exp.py \
    --dataset_name 2wikimultihopqa \
    --split test \
    --gpu_id 0
```

### 3. è‡ªå®šä¹‰OpenAI API

```bash
python run_rpvm_exp.py \
    --dataset_name hotpotqa \
    --split test \
    --gpu_id 0 \
    --openai_api_key your-api-key
```

### 4. CPUè¿è¡Œ

å¦‚æœæ²¡æœ‰GPUæˆ–GPUå†…å­˜ä¸è¶³ï¼š

```bash
# æ–¹æ³•1: å‘½ä»¤è¡ŒæŒ‡å®š
python run_rpvm_exp.py \
    --dataset_name hotpotqa \
    --split test \
    --gpu_id null

# æ–¹æ³•2: ä¿®æ”¹é…ç½®æ–‡ä»¶
# åœ¨rpvm_config.yamlä¸­è®¾ç½®: gpu_id: null
```

---

## ğŸ“Š ç»“æœåˆ†æ

### è¾“å‡ºæ–‡ä»¶

å®éªŒå®Œæˆåï¼Œåœ¨ `RPVM/output/rpvm_experiments/` ç›®å½•ä¸‹ç”Ÿæˆï¼š

#### 1. `intermediate_data.jsonl`

æ¯è¡Œä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´æ¨ç†è¿‡ç¨‹ï¼š

```json
{
  "question": "What is the fight song of the university in Lawrence, Kansas?",
  "iterations": [
    {
      "iteration": 1,
      "plans": [
        "The university in Lawrence, Kansas is the University of Kansas.",
        "The fight song of the University of Kansas is 'I'm a Jayhawk'."
      ],
      "verifications": [
        {
          "plan_index": 1,
          "verdict": "supported",
          "retrievals": 1
        },
        {
          "plan_index": 2,
          "verdict": "contradicted",
          "corrected_plan": "The fight song is 'Kansas Song'.",
          "retrievals": 1
        }
      ],
      "updated_memory": "1. University of Kansas (verified)\n2. Fight song is Kansas Song (corrected)"
    }
  ],
  "final_answer": "Kansas Song",
  "total_retrievals": 2
}
```

#### 2. `metric_score.txt`

è¯„ä¼°æŒ‡æ ‡ï¼š

```
EM: 0.452
F1: 0.589
ACC: 0.623
```

#### 3. `config.yaml`

ä¿å­˜çš„å®Œæ•´é…ç½®ï¼Œä¾¿äºå¤ç°

### åˆ†æä¸­é—´æ•°æ®

ä½¿ç”¨Pythonåˆ†æä¸­é—´æ•°æ®ï¼š

```python
import json

# è¯»å–ä¸­é—´æ•°æ®
with open('output/rpvm_experiments/intermediate_data.jsonl') as f:
    data = [json.loads(line) for line in f]

# ç»Ÿè®¡è¿­ä»£æ¬¡æ•°
iterations = [len(item['iterations']) for item in data]
print(f"å¹³å‡è¿­ä»£æ¬¡æ•°: {sum(iterations)/len(iterations):.2f}")

# ç»Ÿè®¡æ£€ç´¢æ¬¡æ•°
retrievals = [item['total_retrievals'] for item in data]
print(f"å¹³å‡æ£€ç´¢æ¬¡æ•°: {sum(retrievals)/len(retrievals):.2f}")

# ç»Ÿè®¡verdictåˆ†å¸ƒ
verdicts = []
for item in data:
    for iter_info in item['iterations']:
        for ver in iter_info.get('verifications', []):
            verdicts.append(ver['verdict'])

from collections import Counter
print("Verdictåˆ†å¸ƒ:", Counter(verdicts))
```

### å¯è§†åŒ–åˆ†æ

åˆ›å»ºç®€å•çš„ç»Ÿè®¡å›¾è¡¨ï¼š

```python
import matplotlib.pyplot as plt
import json

# è¯»å–æ•°æ®
with open('output/rpvm_experiments/intermediate_data.jsonl') as f:
    data = [json.loads(line) for line in f]

# ç»˜åˆ¶è¿­ä»£æ¬¡æ•°åˆ†å¸ƒ
iterations = [len(item['iterations']) for item in data]
plt.figure(figsize=(10, 4))
plt.hist(iterations, bins=range(1, max(iterations)+2), alpha=0.7)
plt.xlabel('Iteration Count')
plt.ylabel('Frequency')
plt.title('Distribution of Iteration Counts')
plt.savefig('iteration_distribution.png')

# ç»˜åˆ¶æ£€ç´¢æ¬¡æ•°åˆ†å¸ƒ
retrievals = [item['total_retrievals'] for item in data]
plt.figure(figsize=(10, 4))
plt.hist(retrievals, bins=20, alpha=0.7)
plt.xlabel('Retrieval Count')
plt.ylabel('Frequency')
plt.title('Distribution of Retrieval Counts')
plt.savefig('retrieval_distribution.png')
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: OpenAI APIè°ƒç”¨å¤±è´¥

**é”™è¯¯**: `openai.error.AuthenticationError`

**è§£å†³**:
1. æ£€æŸ¥API keyæ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥API keyæ˜¯å¦æœ‰ä½™é¢
3. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
4. å°è¯•è®¾ç½®ä»£ç†ï¼ˆå¦‚éœ€è¦ï¼‰

```python
# åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ä»£ç†è®¾ç½®
openai_setting:
  api_key: "your-key"
  base_url: "https://api.openai.com/v1"
  http_client: 
    proxies: "http://your-proxy:port"
```

### Q2: æ£€ç´¢å™¨åŠ è½½æ…¢æˆ–å¤±è´¥

**é—®é¢˜**: E5æ¨¡å‹æˆ–ç´¢å¼•åŠ è½½æ—¶é—´é•¿

**è§£å†³**:
1. ç¡®ä¿æ¨¡å‹å’Œç´¢å¼•åœ¨æœ¬åœ°ï¼ˆä¸è¦æ¯æ¬¡ä»HuggingFaceä¸‹è½½ï¼‰
2. ä½¿ç”¨SSDå­˜å‚¨ç´¢å¼•æ–‡ä»¶
3. è®¾ç½® `retrieval_use_fp16: True` å‡å°‘å†…å­˜å ç”¨
4. å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹

```bash
# é¢„å…ˆä¸‹è½½E5æ¨¡å‹
from transformers import AutoModel, AutoTokenizer
model = AutoModel.from_pretrained("intfloat/e5-base-v2")
tokenizer = AutoTokenizer.from_pretrained("intfloat/e5-base-v2")
model.save_pretrained("./models/e5-base-v2")
tokenizer.save_pretrained("./models/e5-base-v2")
```

### Q3: GPUå†…å­˜ä¸è¶³

**é”™è¯¯**: `CUDA out of memory`

**è§£å†³**:
1. å‡å°batch size
2. ä½¿ç”¨FP16
3. ä½¿ç”¨CPU
4. æ¸…ç†GPUç¼“å­˜

```python
import torch
torch.cuda.empty_cache()
```

### Q4: æ•°æ®é›†åŠ è½½å¤±è´¥

**é”™è¯¯**: `Dataset not found`

**è§£å†³**:
1. æ£€æŸ¥ `data_dir` è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤æ•°æ®é›†æ–‡ä»¶å­˜åœ¨
3. æ£€æŸ¥æ•°æ®é›†åç§°æ‹¼å†™
4. å‚è€ƒFlashRAGæ–‡æ¡£å‡†å¤‡æ•°æ®é›†

### Q5: ç»“æœä¸é¢„æœŸä¸ç¬¦

**å¯èƒ½åŸå› **:
1. æ£€ç´¢è´¨é‡ä¸é«˜ â†’ è°ƒæ•´ `retrieval_topk`
2. è®¡åˆ’ç”Ÿæˆè´¨é‡å·® â†’ è°ƒæ•´ `planner_temperature`
3. éªŒè¯è¿‡äºå®½æ¾/ä¸¥æ ¼ â†’ è°ƒæ•´ `verifier_temperature`
4. è¿­ä»£æ¬¡æ•°ä¸å¤Ÿ â†’ å¢åŠ  `max_iter`

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰Promptæ¨¡æ¿

ä¿®æ”¹ `rpvm_pipeline.py` ä¸­çš„promptæ„å»ºæ–¹æ³•ï¼š

```python
def _build_planner_prompt(self, question: str, memory: str) -> str:
    # è‡ªå®šä¹‰ä½ çš„prompté€»è¾‘
    custom_prompt = f"""Your custom prompt here
    Question: {question}
    Memory: {memory}
    """
    return custom_prompt
```

### 2. ä½¿ç”¨ä¸åŒçš„LLM

è™½ç„¶å½“å‰å®ç°ä½¿ç”¨OpenAI APIï¼Œä½†å¯ä»¥æ‰©å±•æ”¯æŒå…¶ä»–LLMï¼š

```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­
framework: "hf"  # æˆ– "vllm"
generator_model: "meta-llama/Llama-2-7b-chat-hf"
generator_model_path: "path/to/llama2"
```

éœ€è¦ä¿®æ”¹ `rpvm_pipeline.py` ä¸­çš„æ¶ˆæ¯æ ¼å¼ä»¥é€‚é…ä¸åŒæ¨¡å‹ã€‚

### 3. é›†æˆReranker

æé«˜æ£€ç´¢è´¨é‡ï¼š

```yaml
use_reranker: True
rerank_model_name: "bge-reranker-base"
rerank_model_path: "BAAI/bge-reranker-base"
```

### 4. æ‰¹é‡å®éªŒå¯¹æ¯”

åˆ›å»ºå®éªŒè„šæœ¬å¯¹æ¯”ä¸åŒé…ç½®ï¼š

```python
experiments = [
    {"max_iter": 3, "note": "iter3"},
    {"max_iter": 5, "note": "iter5"},
    {"max_iter": 7, "note": "iter7"},
]

for exp in experiments:
    config_dict = {
        "rpvm_config": {"max_iter": exp["max_iter"]},
        "save_note": f"rpvm_{exp['note']}"
    }
    # è¿è¡Œå®éªŒ...
```

### 5. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°ï¼š

```python
from flashrag.evaluator import Evaluator

class CustomEvaluator(Evaluator):
    def evaluate(self, dataset):
        # è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
        custom_metric = self.calculate_custom_metric(dataset)
        return {"custom": custom_metric, **super().evaluate(dataset)}
```

### 6. é”™è¯¯å¤„ç†å’Œé‡è¯•

æ·»åŠ æ›´robustçš„é”™è¯¯å¤„ç†ï¼š

```python
# åœ¨rpvm_pipeline.pyä¸­æ·»åŠ 
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def _call_generator_with_retry(self, messages, **kwargs):
    try:
        return self.generator.generate([messages], **kwargs)[0]
    except Exception as e:
        print(f"Generator call failed: {e}, retrying...")
        raise
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶è¡Œå¤„ç†

è™½ç„¶å½“å‰å®ç°æ˜¯ä¸²è¡Œçš„ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¹¶è¡ŒåŒ–ï¼š

```python
from concurrent.futures import ThreadPoolExecutor

def process_batch(questions):
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(pipeline._run_single_question, questions))
    return results
```

### 2. ç¼“å­˜æ£€ç´¢ç»“æœ

```yaml
save_retrieval_cache: True
use_retrieval_cache: True  # åœ¨ç¬¬äºŒæ¬¡è¿è¡Œæ—¶è®¾ç½®
retrieval_cache_path: "output/retrieval_cache.json"
```

### 3. å‡å°‘APIè°ƒç”¨

- å¢åŠ  `planner_temperature` ä½¿ç”Ÿæˆæ›´ç¨³å®š
- å‡å°‘ `max_retrieval_attempts` 
- å¯ç”¨ `enable_memory_summary` é¿å…è¿‡é•¿ä¸Šä¸‹æ–‡

---

## ğŸ“š å‚è€ƒèµ„æº

- [FlashRAGæ–‡æ¡£](../docs/)
- [E5æ¨¡å‹](https://huggingface.co/intfloat/e5-base-v2)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- [HotpotQAæ•°æ®é›†](https://hotpotqa.github.io/)
- [2WikiMultihopQAæ•°æ®é›†](https://github.com/Alab-NII/2wikimultihop)

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä»å°è§„æ¨¡å¼€å§‹**: å…ˆç”¨5-10ä¸ªæ ·æœ¬æµ‹è¯•
2. **ç›‘æ§APIæ¶ˆè€—**: æ³¨æ„OpenAI APIçš„tokenä½¿ç”¨å’Œæˆæœ¬
3. **ä¿å­˜ä¸­é—´ç»“æœ**: å¯ç”¨ `save_intermediate_data` ä¾¿äºè°ƒè¯•
4. **ç‰ˆæœ¬æ§åˆ¶é…ç½®**: æ¯æ¬¡å®éªŒä¿å­˜é…ç½®æ–‡ä»¶
5. **è®°å½•å®éªŒæ—¥å¿—**: è®°å½•ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°
6. **å®šæœŸå¤‡ä»½**: å¤‡ä»½é‡è¦çš„å®éªŒç»“æœå’Œç´¢å¼•æ–‡ä»¶

---

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ [README.md](README.md) æˆ–æŸ¥çœ‹ [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜) éƒ¨åˆ†ã€‚
